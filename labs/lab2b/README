NAME: Tanzeela Khan
EMAIL: tanzeelakhan@g.ucla.edu
ID: 204577214

---FILES---

lab2_add.c: Uses pthreads to add to a shared variable. Allows for enabling of different locks to stop race conditions.
SortedList.h: Supplied header file that describes the interfaces for the sorted linked list.
SortedList.c: Implements functions in Sorted List Header field to insert, delete, and lookup in the doubly linked list. 
Makefile: Builds deliverable wtih tests, profile, grahps, dist, and clean. 
lab2_list.c: Uses pthreads to add to a shared Sorted list. Allows for enabling of different locks to stop race conditions. 
lab2_add.csv: generated graph input from Makefile tests. lab2_add.gp uses this data to plot on gnuplot.
lab2_list.csv: generated graph input from Makefile tests. lab2_list.gp uses this data to plot on gnuplot.
lab2_add-1.png: threads and iterations required to generate a failure (with and without yields)
lab2_add-2.png: average time per operation with and without yields.
lab2_add-3.png: average time per (single threaded) operation vs. the number of iterations.
lab2_add-4.png: threads and iterations that can run successfully with yields under each of the synchronization options.
lab2_add-5.png: average time per (protected) operation vs. the number of threads.
lab2_list-1.png: average time per (single threaded) unprotected operation vs. number of iterations (illustrating the correction of the per-operation cost for the list length).
lab2_list-2.png: threads and iterations required to generate a failure (with and without yields).
lab2_list-3.png: iterations that can run (protected) without failure.
lab2_list-4.png: (length-adjusted) cost per operation vs the number of threads for the various synchronization options.


---QUESTIONS---

QUESTION 2.3.1 - Cycles in the basic list implementation:

Where do you believe most of the cycles are spent in the 1 and 2-thread list tests?
Most of the cycles will be spent in the list (inserting, deleting, looking up) options during the 1 and 2 thread tests. 

Why do you believe these to be the most expensive parts of the code?
Since there are only a few threads, there won't be a lot of time spent on context switching, but a higher percentage of time dedicated to the actual operations.

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
When there are more threads, there are more context switches, so more time would be spent spinning and waiting for other threads to release the spin locks.

Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
Since there are more threads, there will be more context swiches. So more time will be dedicated to making those system calls to switch between kernal and user mode to schedule the competing threads.

QUESTION 2.3.2 - Execution Profiling:

Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
The function threadadd is consuming most of the cyles when the psin lock verion of hte list experciser is run. 

Why does this operation become so expensive with large numbers of threads?
Since a large number of threads are trying to execute the protected critical section, multiple threads are spinnning and using up CPU resources. 

QUESTION 2.3.3 - Mutex Wait Time:

Look at the average time per operation (vs # threads) and the average wait-for-mutex time (vs #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
The average lock-wait time rises so dramatically with the number of contending threads because more threads are in the queue when one thread has locked a critical section. Naturally, wait time will be longer.

Why does the completion time per operation rise (less dramatically) with the number of contending threads?
The completion time per operation rises less dramatically with thenumber of contending threads because it does not include the waiting time for threasd, only the processesing to competle the tasks. Thet ime is high for unaccounted for waiting time. 

How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
It is possible for the wait time per operation to go up faster than the completion time per operation because wait tim eis is sum of twthe wait times for all ther threasd, including the lcoking time. Completion time does not include t he wiat time for all the threasd, so it can be less.

QUESTION 2.3.4 - Performance of Partitioned Lists

Explain the change in performance of the synchronized methods as a function of the number of lists.
The increasing number of lists does not affect the amount of time threads spend waiting to acquire the lock because the thread acquires the lock before performing any of the list operations. 

Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
The throughput instead increases with the number of lists because elements are spread through the lists, making them shorter. The time required to perform the basic list operations (insert, delete, and lookup) are much shorter.

It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.
In the above curves, it does not appear that the throughout of an N-way partitioned lists results in a thoroughput of a single list with fewer 1/N threads. the throroughput mainly depends on how mnay threads are competing in the critical section. Even if the nw-way paritioned lists have more threads, the threds do not spend as much tim eint he ciritcal seciton and have an oreall higher throughoput. 


---SOURCES---

CS111 TA Slides
"Random String generator in C", AntonioCS, https://codereview.stackexchange.com/questions/29198/random-string-generator-in-c
"Pthreads Overview", Lawrence Livermore National Laboratory, https://computing.llnl.gov/tutorials/pthreads/#Designing
"Built-in functions for atomic memory access", gcc.gnu.org, https://gcc.gnu.org/onlinedocs/gcc-4.4.5/gcc/Atomic-Builtins.html
"clock_gettime(3) - Linux man page", linux.die.net, https://linux.die.net/man/3/clock_gettime
"Simple hash functions", Hardell, http://stackoverflow.com/questions/14409466/simple-hash-functions

